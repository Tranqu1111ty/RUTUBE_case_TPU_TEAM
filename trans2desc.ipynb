{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e75100e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pymorphy2\n",
    "import re\n",
    "import nltk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79d38e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_timestamps(text):\n",
    "    text = text.split(\"] \")[1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def ret_stt(stt_name):\n",
    "    with open(f\"./train_stt/{stt_name}\", 'r', encoding=\"utf_8_sig\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [del_timestamps(line.strip()) for line in lines]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83bb6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus_(data, corpus_index):\n",
    "    data[\"stt\"] = data[\"stt_name\"].apply(ret_stt)\n",
    "\n",
    "    txt = data[\"stt\"][corpus_index]\n",
    "    \n",
    "    tmp = []\n",
    "    flag = 0\n",
    "    flag0 = 0\n",
    "    \n",
    "    for i in txt:\n",
    "        \n",
    "        for j in i:\n",
    "            if j == '*':\n",
    "                flag = 1\n",
    "            else:\n",
    "                continue\n",
    "        if flag == 0 and len(i.split()) > 5:\n",
    "            tmp.append(i)\n",
    "        else:\n",
    "            flag = 0\n",
    "\n",
    "    \n",
    "    return pd.Series(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f668a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.head(20)\n",
    "    data[\"stt_sum\"] = [process_corpus_(data, i) for i in range(data.shape[0])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "792b8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.text = X\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=150)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        output = self.text[index]\n",
    "        output = self.tokenize(output)\n",
    "        return {k: v.reshape(-1) for k, v in output.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ca5d7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output['last_hidden_state']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1e5f64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(50364, 1536)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50364, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 24)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(50364, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 24)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (o): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=50364, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('content/rubert_cased_L-12_H-768_A-12_pt')\n",
    "model = BertModel.from_pretrained('content/rubert_cased_L-12_H-768_A-12_pt', output_hidden_states=True)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained('ai-forever/FRED-T5-1.7B',eos_token='</s>')\n",
    "model1 = T5ForConditionalGeneration.from_pretrained('ai-forever/FRED-T5-1.7B')\n",
    "device='cpu'\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39fe940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(corpus):\n",
    "    eval_ds = CustomDataset(corpus)\n",
    "    eval_dataloader = DataLoader(eval_ds, batch_size=10)\n",
    "    if len(corpus) > 50:\n",
    "\n",
    "        embeddings = torch.Tensor().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for n_batch, batch in enumerate(tqdm(eval_dataloader)):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                embeddings = torch.cat([embeddings, mean_pooling(outputs, batch['attention_mask'])])\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "        pca = PCA(n_components=15, random_state=42)\n",
    "        emb_15d = pca.fit_transform(embeddings)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(emb_15d)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "        cluster_centers_indices = {}\n",
    "        for cluster_label in unique_clusters:\n",
    "            cluster_centers_indices[cluster_label] = np.where(cluster_labels == cluster_label)[0][0]\n",
    "\n",
    "        tmp = []\n",
    "        for i in cluster_centers_indices.values():\n",
    "            tmp.append(i)\n",
    "        tmp.sort()\n",
    "\n",
    "        line1 = \"\"\n",
    "        core_sentences = []\n",
    "        for i in range(len(tmp)//2):\n",
    "            line1 = line1 + corpus[tmp[i]] + f\" <extra_id_{i}>\"\n",
    "            core_sentences.append(corpus[tmp[i]])\n",
    "\n",
    "        line2 = \"\"\n",
    "        for i in range(len(tmp)//2, len(tmp)):\n",
    "            line2 = line2 + corpus[tmp[i]] + f\" <extra_id_{i - len(tmp)//2}>\"\n",
    "            core_sentences.append(corpus[tmp[i]])\n",
    "\n",
    "        lm_text=\" : \" + line1\n",
    "        input_ids=torch.tensor([tokenizer1.encode(lm_text)]).to(device)\n",
    "        outputs=model1.generate(input_ids,eos_token_id=tokenizer1.eos_token_id,early_stopping=True)\n",
    "        replace_dict = {\n",
    "            match.group(): replacement\n",
    "            for match, replacement in zip(re.finditer(r'<extra_id_\\d+>', tokenizer1.decode(outputs[0][1:])),\n",
    "                                          re.split(r'<extra_id_\\d+>', tokenizer1.decode(outputs[0][1:]))[1:])\n",
    "        }\n",
    "\n",
    "        def replacer(match):\n",
    "            return replace_dict.get(match.group(), '')\n",
    "\n",
    "        result1 = re.sub(r'<extra_id_\\d+>', replacer, line1)\n",
    "\n",
    "        lm_text=\" : \" + line2\n",
    "        input_ids=torch.tensor([tokenizer1.encode(lm_text)]).to(device)\n",
    "        outputs=model1.generate(input_ids,eos_token_id=tokenizer1.eos_token_id,early_stopping=True)\n",
    "        replace_dict = {\n",
    "        match.group(): replacement\n",
    "            for match, replacement in zip(re.finditer(r'<extra_id_\\d+>', tokenizer1.decode(outputs[0][1:])),\n",
    "                                          re.split(r'<extra_id_\\d+>', tokenizer1.decode(outputs[0][1:]))[1:])\n",
    "        }\n",
    "\n",
    "        def replacer(match):\n",
    "            return replace_dict.get(match.group(), '')\n",
    "\n",
    "        result2 = re.sub(r'<extra_id_\\d+>', replacer, line2)\n",
    "\n",
    "        final_line = result1 + result2\n",
    "\n",
    "        return final_line\n",
    "    \n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46b826ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(cluster, n):\n",
    "    print(corpus[emb_2d['label'] == cluster][::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49c1c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_metric(text, text_sum):\n",
    "    if isinstance(text_sum, str):\n",
    "        return round(meteor([word_tokenize(text)],word_tokenize(text_sum)), 4)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13d15e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_metric(reference, hypothesis):\n",
    "    reference = [word_tokenize(reference)]\n",
    "    hypothesis = word_tokenize(hypothesis)\n",
    "    return round(sentence_bleu(reference, hypothesis), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd4edc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nist_metric(reference, hypothesis):\n",
    "    try:\n",
    "        reference = [word_tokenize(reference)]\n",
    "        hypothesis = word_tokenize(hypothesis)\n",
    "        return round(sentence_nist(reference, hypothesis), 4)\n",
    "    except ZeroDivisionError:\n",
    "        return 0  #     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe325a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\druzh\\AppData\\Local\\Temp\\ipykernel_4700\\2763201655.py:23: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  return pd.Series(tmp)\n"
     ]
    }
   ],
   "source": [
    "data = process_corpus(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "106cdbf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:50<00:00,  1.81s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 14/14 [00:24<00:00,  1.76s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 16/16 [00:26<00:00,  1.65s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 43/43 [01:09<00:00,  1.62s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 10/10 [00:14<00:00,  1.47s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 14/14 [00:21<00:00,  1.53s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 26/26 [00:42<00:00,  1.62s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 46/46 [01:09<00:00,  1.52s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 25/25 [00:40<00:00,  1.62s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 21/21 [00:32<00:00,  1.57s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:17<00:00,  1.58s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 17/17 [00:26<00:00,  1.58s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 11/11 [00:16<00:00,  1.54s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 14/14 [00:21<00:00,  1.56s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 12/12 [00:17<00:00,  1.46s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 61/61 [01:37<00:00,  1.59s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|| 18/18 [00:27<00:00,  1.51s/it]\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data[\"desc_proc\"] = [calc(i) for i in data[\"stt_sum\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6358eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>stt_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>stt</th>\n",
       "      <th>stt_sum</th>\n",
       "      <th>desc_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.mp4</td>\n",
       "      <td>0.txt</td>\n",
       "      <td></td>\n",
       "      <td>  I #3</td>\n",
       "      <td>        !...</td>\n",
       "      <td>[    ,  ,   ...</td>\n",
       "      <td>0          ,  , ...</td>\n",
       "      <td>   ,  ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.mp4</td>\n",
       "      <td>1.txt</td>\n",
       "      <td>/</td>\n",
       "      <td>   |  2</td>\n",
       "      <td>         ...</td>\n",
       "      <td>[      ,   2 ,  ...</td>\n",
       "      <td>0                                ...</td>\n",
       "      <td>      ?   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.mp4</td>\n",
       "      <td>2.txt</td>\n",
       "      <td></td>\n",
       "      <td> |  4 |    | ...</td>\n",
       "      <td>,    !    ...</td>\n",
       "      <td>[  ,  ,    ...</td>\n",
       "      <td>0        ,  ,   ...</td>\n",
       "      <td> ,  ,    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.mp4</td>\n",
       "      <td>3.txt</td>\n",
       "      <td></td>\n",
       "      <td>   - </td>\n",
       "      <td>  .   ...</td>\n",
       "      <td>[  ,    ....</td>\n",
       "      <td>Series([], dtype: float64)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.mp4</td>\n",
       "      <td>4.txt</td>\n",
       "      <td></td>\n",
       "      <td>.  3.  + Mika Vino</td>\n",
       "      <td>      II?  ...</td>\n",
       "      <td>[  ,   ,  ...</td>\n",
       "      <td>0       ,  !   ...</td>\n",
       "      <td>,  !   ,  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  video_name stt_name category_name  \\\n",
       "0      0.mp4    0.txt      \n",
       "1      1.mp4    1.txt    /   \n",
       "2      2.mp4    2.txt            \n",
       "3      3.mp4    3.txt      \n",
       "4      4.mp4    4.txt        \n",
       "\n",
       "                                               title  \\\n",
       "0                                 I #3   \n",
       "1                        |  2   \n",
       "2   |  4 |    | ...   \n",
       "3                      -    \n",
       "4      .  3.  + Mika Vino   \n",
       "\n",
       "                                         description  \\\n",
       "0          !...   \n",
       "1           ...   \n",
       "2  ,    !    ...   \n",
       "3    .   ...   \n",
       "4        II?  ...   \n",
       "\n",
       "                                                 stt  \\\n",
       "0  [    ,  ,   ...   \n",
       "1  [      ,   2 ,  ...   \n",
       "2  [  ,  ,    ...   \n",
       "3  [  ,    ....   \n",
       "4  [  ,   ,  ...   \n",
       "\n",
       "                                             stt_sum  \\\n",
       "0  0          ,  , ...   \n",
       "1  0                                ...   \n",
       "2  0        ,  ,   ...   \n",
       "3                         Series([], dtype: float64)   \n",
       "4  0       ,  !   ...   \n",
       "\n",
       "                                           desc_proc  \n",
       "0      ,  ,   ...  \n",
       "1         ?   ...  \n",
       "2    ,  ,    ...  \n",
       "3                                                     \n",
       "4   ,  !   ,  ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f4d6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"met\"] = data.apply(lambda x: meteor_metric(x.description, x.desc_proc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f602a770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11294500000000003"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.met.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07446a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\druzh\\Project_python\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "data[\"bleu\"] = data.apply(lambda x: bleu_metric(x.description, x.desc_proc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb8007db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nist\"] = data.apply(lambda x: nist_metric(x.description, x.desc_proc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "660cb7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4623, 0.3914, 0.6305, 0.    , 0.6475, 0.0615, 0.4222, 0.8587,\n",
       "       0.345 , 1.0224, 0.    , 0.    , 0.5519, 0.5458, 0.8412, 0.5266,\n",
       "       0.532 , 0.5068, 1.3808, 0.4985])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"nist\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d8682cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.511255"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nist.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
